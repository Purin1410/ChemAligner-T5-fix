t5:
  pretrained_model_name_or_path: "QizhiPei/biot5-plus-base"

trainer_init:
  seed_everything: 2183
  cuda: true
  deterministic: true
  strategy: "ddp_find_unused_parameters_true"

 # Checkpoint callback
  output_folder: "output/chebi20_train"
  filename: "ckpt_{epoch}_{eval_loss}"
  save_top_k: 3
  verbose: true
  monitor: "avg_metrics" # turn to "eval_loss" if "run_text2mol_metrics" = False
  mode: "max" # turn to "min" if "run_text2mol_metrics" = False

  max_epochs: 100
  num_devices: 4
  precision: "32"

  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  grad_accum: 8

  optimizer:
    name: "adamw"          # "adamw" or "sgd"
    weight_decay: 1e-4
    momentum: 0.9          # used only for SGD

  # Scheduler init
  lr: 5e-5
  warmup_ratio: 0.0

  optuna:
    activate: false
    lr_sweep: [5e-5, 1e-4, 2e-4, 5e-4]
    n_trials: 10

  wandb:
    project: "ACL_Lang2Mol"
    name: "chebi20_train"
  

dataset_init:
  dataset_name: "chebi-20"
  task: "lang2mol"
  train_batch_size: 8
  eval_batch_size: 64
  num_workers: 24
  pin_memory: True
  persistent_workers: True

method_init:
  method: "chemaligner" #base, chemaligner
  seq2seq_loss_weight: 1.0
  contrastive_loss_weight: 0.3

eval_init:
  # Generation config
  max_length: 512
  num_beams: 1
  use_amp: true
  max_samples: 0          # 0 means unlimited
  chunk_size: 1024
  compute_fcd: false
  run_text2mol_metrics: true
  # Init for evaluate after final training
  run_evaluate_after_trainning_done: false
  config_file: None # if None, it will choose chebi20_eval.yaml as default
